{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename) as opened_file:\n",
    "        data = [tuple(line.split(\"\\t\")) for line in opened_file]\n",
    "    return [datum[0] for datum in data], [int(datum[1]) for datum in data]\n",
    "\n",
    "class LogisticRegressionModel(object):\n",
    "    def __init__(self, embedder):\n",
    "        # embedder to bedzie klasa ktora przez was bedzie napisana\n",
    "        self.embedder = embedder\n",
    "        self.model = LogisticRegression()\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        # tutaj nastepuje uczenie embeddingu\n",
    "        self.embedder.train_embeddings(X)\n",
    "        embedded = [self.embedder.embed(x) for x in X]\n",
    "        # upewnienie sie ze embedding ma staly wymiar.\n",
    "        # Nie przejscie tej asercji oznacza niezaliczenie zadania\n",
    "        assert(len(set(len(embedding) for embedding in embedded))==1)\n",
    "        self.model.fit(embedded, Y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        embedded = [self.embedder.embed(x) for x in X]\n",
    "        # j.w.\n",
    "        assert(len(set(len(embedding) for embedding in embedded))==1)\n",
    "        return self.model.predict(embedded)\n",
    "\n",
    "    def score(self, X_test, Y_test):\n",
    "        assert(len(X_test)==len(Y_test))\n",
    "        predictions = self.predict(X_test)\n",
    "        matching = sum(y1==y2 for y1, y2 in zip(predictions, Y_test))\n",
    "        return matching/len(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_data('train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import uuid\n",
    "\n",
    "#Przemysław Onak\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class Embedder(object):\n",
    "    def __init__(self, sentence_length = 25, embed_dimension = 250, l1_fil = 64, l1_ker = 5, epochs = 5):        \n",
    "            self.sentence_length = sentence_length\n",
    "            self.embed_dimension = embed_dimension\n",
    "            self.l1_fil = l1_fil\n",
    "            self.l1_ker = l1_ker\n",
    "            self.epochs = 5\n",
    "    \n",
    "    # zamienia zdanie na reprezentację za pomocą wektora z indeksami w bag of words\n",
    "    def process_string(self, sentence):\n",
    "        analyzer = self.vectorizer.build_analyzer()\n",
    "        result = [self.word_dict[word] for word in analyzer(sentence) if word in self.word_dict]\n",
    "        result = result[:self.sentence_length] # unicemy zdania dłuższe niż sentence length\n",
    "        result.extend([0] * (self.sentence_length - len(result))) #padowanie do self.sentence_length\n",
    "        return result\n",
    "    \n",
    "    def train_embeddings(self, data):\n",
    "        \"\"\"\n",
    "        Trains the embedder on the given data.\n",
    "        The data is a list of sentences, each sentence is a single string.\n",
    "        \"\"\" \n",
    "        self.vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             lowercase = True,    \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = None) \n",
    "        self.vectorizer.fit(data)\n",
    "        self.word_dict = self.vectorizer.vocabulary_\n",
    "        \n",
    "        NUM_WORDS=len(self.word_dict)\n",
    "        self.NUM_WORDS = NUM_WORDS\n",
    "        \n",
    "        # tablica z wektorowymi reprezentacjami zdań \n",
    "        train = [self.process_string(sentence) for sentence in data]\n",
    "         \n",
    "        # model tensorflow c&w    \n",
    "        inputs = tf.placeholder(tf.int32, shape=[None, self.sentence_length])\n",
    "        false_inputs = tf.placeholder(tf.int32, shape=[None, self.sentence_length])\n",
    "        embedding_matrix = tf.Variable(tf.random_uniform([NUM_WORDS, self.embed_dimension], -1.0, 1.0), name=\"embedder\")\n",
    "\n",
    "        def get_output_tensor(scope, input_tensor):\n",
    "            with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "                one_hot = tf.one_hot(input_tensor, depth=NUM_WORDS)\n",
    "\n",
    "                # poniewaz tensorflow nie obsluguje broadcastu mnozenia macierzy,\n",
    "                # musimy pomanipulowac naszym tensorem\n",
    "                reshaped = tf.reshape(one_hot, (-1, NUM_WORDS))\n",
    "                embedded = tf.matmul(reshaped, embedding_matrix)\n",
    "                embedded = tf.reshape(embedded, (-1, self.sentence_length, self.embed_dimension))\n",
    "\n",
    "                prev = embedded\n",
    "                \n",
    "                # dodajemy warstwy konwolucji, po ktorych następuje maxpool\n",
    "                layer1 = tf.layers.conv1d(prev, self.l1_fil, self.l1_ker, padding='same', activation=tf.nn.relu)\n",
    "                prev = layer1\n",
    "                \n",
    "                #layer2 = tf.layers.conv1d(prev, 64, 3, padding='same', activation=tf.nn.relu)\n",
    "                #prev = layer2\n",
    "\n",
    "                pooling = tf.layers.max_pooling1d(prev, 3, 2)\n",
    "\n",
    "                # zeby polaczyc kanaly wyplaszczamy dane, a nastepnie wrzucamy\n",
    "                # do zwyklej sieci fully connected\n",
    "                flattened = tf.contrib.layers.flatten(pooling)\n",
    "                layer3 = tf.layers.dense(flattened, 50)\n",
    "                score = tf.layers.dense(layer3, 1)\n",
    "                return score\n",
    "        \n",
    "        scope = str(uuid.uuid4())\n",
    "        scorer1 = get_output_tensor(scope, inputs)\n",
    "        scorer2 = get_output_tensor(scope, false_inputs)\n",
    "\n",
    "        loss = tf.reduce_mean(tf.maximum(tf.zeros_like(scorer1), 1 - (scorer1 - scorer2)))\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1e-2).minimize(loss)\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        \n",
    "        def chunk_once(doc, width):\n",
    "            chunks = [doc[i:i+width] for i in range(0, len(doc), width)]\n",
    "            chunks[-1] += [0] * (width - len(chunks[-1]))\n",
    "            return chunks\n",
    "\n",
    "        def chunk_and_padd(documents, width):\n",
    "            chunks = [w for d in documents for w in chunk_once(d, width)]\n",
    "            return chunks\n",
    "        \n",
    "        # kroi tablice na podtablice długości n\n",
    "        dataset = chunk_and_padd(train, self.sentence_length)\n",
    "\n",
    "        def noiser(sentence, p):\n",
    "            new_sent = [x for x in sentence]\n",
    "            for i, _ in enumerate(new_sent):\n",
    "                if np.random.random() < p:\n",
    "                    new_sent[i] = np.random.choice(NUM_WORDS-1)+1\n",
    "            return new_sent\n",
    "        epochs = self.epochs\n",
    "        batch_size = 256\n",
    "        num_batches = len(dataset)//batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            #print('epoch: ' + str(epoch + 1) + \"/\" + str(epochs) )\n",
    "            for _ in range(num_batches):\n",
    "                batch_indexes = np.random.choice(len(dataset),batch_size)\n",
    "                batch = [dataset[i] for i in batch_indexes]\n",
    "                false_batch = [noiser(dataset[i],0.2) for i in batch_indexes]\n",
    "                sess.run(optimizer,feed_dict={inputs:batch, false_inputs:false_batch})\n",
    "\n",
    "        matrix = sess.run(embedding_matrix)\n",
    "        self.embedding = matrix\n",
    "        pass\n",
    "\n",
    "    def embed(self, sentence):\n",
    "        vectorized = self.vectorizer.transform([sentence]).toarray()[0]\n",
    "        result = vectorized.dot(self.embedding)\n",
    "        return result\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit start\n",
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[CV] embed_dimension=10, l1_fil=16, l1_ker=3, sentence_length=20 .....\n",
      "WARNING:tensorflow:From C:\\Users\\Przemek\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "WARNING:tensorflow:From C:\\Users\\Przemek\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "0.424821897263 <bound method Tester.get_params of <__main__.Tester object at 0x000001C685672198>>\n",
      "0.44196512282 <bound method Tester.get_params of <__main__.Tester object at 0x000001C685672198>>\n",
      "[CV]  embed_dimension=10, l1_fil=16, l1_ker=3, sentence_length=20, score=0.42482189726284214, total= 2.4min\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.4min remaining:    0.0s\n",
      "[CV] embed_dimension=10, l1_fil=16, l1_ker=3, sentence_length=20 .....\n",
      "0.494188226472 <bound method Tester.get_params of <__main__.Tester object at 0x000001C69744A320>>\n",
      "0.485280330021 <bound method Tester.get_params of <__main__.Tester object at 0x000001C69744A320>>\n",
      "[CV]  embed_dimension=10, l1_fil=16, l1_ker=3, sentence_length=20, score=0.49418822647169103, total= 2.2min\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  4.7min remaining:    0.0s\n",
      "[CV] embed_dimension=10, l1_fil=16, l1_ker=3, sentence_length=20 .....\n",
      "0.490622655664 <bound method Tester.get_params of <__main__.Tester object at 0x000001C69744D8D0>>\n",
      "0.478627671541 <bound method Tester.get_params of <__main__.Tester object at 0x000001C69744D8D0>>\n",
      "[CV]  embed_dimension=10, l1_fil=16, l1_ker=3, sentence_length=20, score=0.490622655663916, total= 2.2min\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  6.9min remaining:    0.0s\n",
      "[CV] embed_dimension=10, l1_fil=16, l1_ker=3, sentence_length=30 .....\n",
      "0.447694038245 <bound method Tester.get_params of <__main__.Tester object at 0x000001C6997CC7B8>>\n",
      "0.459216201013 <bound method Tester.get_params of <__main__.Tester object at 0x000001C6997CC7B8>>\n",
      "[CV]  embed_dimension=10, l1_fil=16, l1_ker=3, sentence_length=30, score=0.44769403824521936, total= 3.4min\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 10.3min remaining:    0.0s\n",
      "[CV] embed_dimension=10, l1_fil=16, l1_ker=3, sentence_length=30 .....\n",
      "0.453318335208 <bound method Tester.get_params of <__main__.Tester object at 0x000001C69743A828>>\n",
      "0.44927807988 <bound method Tester.get_params of <__main__.Tester object at 0x000001C69743A828>>\n",
      "[CV]  embed_dimension=10, l1_fil=16, l1_ker=3, sentence_length=30, score=0.453318335208099, total= 3.4min\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 13.8min remaining:    0.0s\n",
      "[CV] embed_dimension=10, l1_fil=16, l1_ker=3, sentence_length=30 .....\n",
      "0.457239309827 <bound method Tester.get_params of <__main__.Tester object at 0x000001C69846C080>>\n",
      "0.445256842895 <bound method Tester.get_params of <__main__.Tester object at 0x000001C69846C080>>\n",
      "[CV]  embed_dimension=10, l1_fil=16, l1_ker=3, sentence_length=30, score=0.45723930982745686, total= 3.2min\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 17.0min remaining:    0.0s\n",
      "[CV] embed_dimension=10, l1_fil=16, l1_ker=3, sentence_length=50 .....\n",
      "0.472815898013 <bound method Tester.get_params of <__main__.Tester object at 0x000001C69846C080>>\n",
      "0.491093193325 <bound method Tester.get_params of <__main__.Tester object at 0x000001C69846C080>>\n",
      "[CV]  embed_dimension=10, l1_fil=16, l1_ker=3, sentence_length=50, score=0.4728158980127484, total= 5.3min\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 22.3min remaining:    0.0s\n",
      "[CV] embed_dimension=10, l1_fil=16, l1_ker=3, sentence_length=50 .....\n",
      "0.47581552306 <bound method Tester.get_params of <__main__.Tester object at 0x000001C69846C080>>\n",
      "0.45659103694 <bound method Tester.get_params of <__main__.Tester object at 0x000001C69846C080>>\n",
      "[CV]  embed_dimension=10, l1_fil=16, l1_ker=3, sentence_length=50, score=0.47581552305961755, total= 5.2min\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 27.6min remaining:    0.0s\n",
      "[CV] embed_dimension=10, l1_fil=16, l1_ker=3, sentence_length=50 .....\n",
      "0.445986496624 <bound method Tester.get_params of <__main__.Tester object at 0x000001C69846C080>>\n",
      "0.438320209974 <bound method Tester.get_params of <__main__.Tester object at 0x000001C69846C080>>\n",
      "[CV]  embed_dimension=10, l1_fil=16, l1_ker=3, sentence_length=50, score=0.445986496624156, total= 5.2min\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 32.8min remaining:    0.0s\n",
      "[CV] embed_dimension=10, l1_fil=16, l1_ker=5, sentence_length=20 .....\n",
      "0.422197225347 <bound method Tester.get_params of <__main__.Tester object at 0x000001C69846C080>>\n",
      "0.447777986124 <bound method Tester.get_params of <__main__.Tester object at 0x000001C69846C080>>\n",
      "[CV]  embed_dimension=10, l1_fil=16, l1_ker=5, sentence_length=20, score=0.42219722534683163, total= 2.2min\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 35.0min remaining:    0.0s\n",
      "[CV] embed_dimension=10, l1_fil=16, l1_ker=5, sentence_length=20 .....\n",
      "0.45406824147 <bound method Tester.get_params of <__main__.Tester object at 0x000001C69846C080>>\n",
      "0.455465966623 <bound method Tester.get_params of <__main__.Tester object at 0x000001C69846C080>>\n",
      "[CV]  embed_dimension=10, l1_fil=16, l1_ker=5, sentence_length=20, score=0.4540682414698163, total= 2.1min\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed: 37.2min remaining:    0.0s\n",
      "[CV] embed_dimension=10, l1_fil=16, l1_ker=5, sentence_length=20 .....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "\n",
    "\n",
    "class Tester(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params = {}\n",
    "        \n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        self.model = LogisticRegressionModel(Embedder(**self.params))\n",
    "        self.model.fit(X, Y)\n",
    "        pass\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        score = self.model.score(X, Y)\n",
    "        print(score, self.get_params)\n",
    "        return score\n",
    "        pass \n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        self.params = params\n",
    "        pass\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return self.params\n",
    "\n",
    "model = Tester()\n",
    "parameters = {\n",
    "    'sentence_length': [20, 30, 50],\n",
    "    'embed_dimension': [10, 50, 100],\n",
    "    'l1_fil': [16, 32, 64, 128],\n",
    "    'l1_ker': [3, 5, 7, 9]\n",
    "}\n",
    "clf = GridSearchCV(model, parameters, verbose=9999)\n",
    "print('Fit start')\n",
    "clf.fit(X, Y)\n",
    "print('Fit done')\n",
    "\n",
    "print('Najlepsze parametry:')\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.659821428571\n",
      "0.616666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "model = LogisticRegressionModel(Embedder())\n",
    "model.fit(X_train, Y_train)\n",
    "print(model.score(X_train, Y_train))\n",
    "print(model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
