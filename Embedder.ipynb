{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename) as opened_file:\n",
    "        data = [tuple(line.split(\"\\t\")) for line in opened_file]\n",
    "    return [datum[0] for datum in data], [int(datum[1]) for datum in data]\n",
    "\n",
    "class LogisticRegressionModel(object):\n",
    "    def __init__(self, embedder):\n",
    "        # embedder to bedzie klasa ktora przez was bedzie napisana\n",
    "        self.embedder = embedder\n",
    "        self.model = LogisticRegression()\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        # tutaj nastepuje uczenie embeddingu\n",
    "        self.embedder.train_embeddings(X)\n",
    "        embedded = [self.embedder.embed(x) for x in X]\n",
    "        # upewnienie sie ze embedding ma staly wymiar.\n",
    "        # Nie przejscie tej asercji oznacza niezaliczenie zadania\n",
    "        assert(len(set(len(embedding) for embedding in embedded))==1)\n",
    "        self.model.fit(embedded, Y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        embedded = [self.embedder.embed(x) for x in X]\n",
    "        # j.w.\n",
    "        assert(len(set(len(embedding) for embedding in embedded))==1)\n",
    "        return self.model.predict(embedded)\n",
    "\n",
    "    def score(self, X_test, Y_test):\n",
    "        assert(len(X_test)==len(Y_test))\n",
    "        predictions = self.predict(X_test)\n",
    "        matching = sum(y1==y2 for y1, y2 in zip(predictions, Y_test))\n",
    "        return matching/len(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_data('train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class Embedder(object):\n",
    "    def __init__(self, sentence_length = 25, embed_dimension = 100):        \n",
    "            self.sentence_length = sentence_length\n",
    "            self.embed_dimension = embed_dimension\n",
    "        pass\n",
    "    \n",
    "    # zamienia zdanie na reprezentację za pomocą wektora z indeksami w bag of words\n",
    "    def process_string(self, sentence):\n",
    "        analyzer = self.vectorizer.build_analyzer()\n",
    "        result = [self.word_dict[word] for word in analyzer(sentence) if word in self.word_dict]\n",
    "        result = result[:self.sentence_length] # unicemy zdania dłuższe niż sentence length\n",
    "        result.extend([0] * (self.sentence_length - len(result))) #padowanie do self.sentence_length\n",
    "        return result\n",
    "    \n",
    "    def train_embeddings(self, data):\n",
    "        \"\"\"\n",
    "        Trains the embedder on the given data.\n",
    "        The data is a list of sentences, each sentence is a single string.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             lowercase = True,    \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = None) \n",
    "        self.vectorizer.fit(data)\n",
    "        self.word_dict = self.vectorizer.vocabulary_\n",
    "        \n",
    "        NUM_WORDS=len(self.word_dict)\n",
    "        self.NUM_WORDS = NUM_WORDS\n",
    "        \n",
    "        # tablica z wektorowymi reprezentacjami zdań \n",
    "        train = [self.process_string(sentence) for sentence in data]\n",
    "         \n",
    "        # model tensorflow c&w    \n",
    "        inputs = tf.placeholder(tf.int32, shape=[None, self.sentence_length])\n",
    "        false_inputs = tf.placeholder(tf.int32, shape=[None, self.sentence_length])\n",
    "        embedding_matrix = tf.Variable(tf.random_uniform([NUM_WORDS, self.embed_dimension], -1.0, 1.0), name=\"embedder\")\n",
    "\n",
    "        def get_output_tensor(input_tensor):\n",
    "            with tf.variable_scope('name', reuse=tf.AUTO_REUSE):\n",
    "                one_hot = tf.one_hot(input_tensor, depth=NUM_WORDS)\n",
    "\n",
    "                # poniewaz tensorflow nie obsluguje broadcastu mnozenia macierzy,\n",
    "                # musimy pomanipulowac naszym tensorem\n",
    "                reshaped = tf.reshape(one_hot, (-1, NUM_WORDS))\n",
    "                embedded = tf.matmul(reshaped, embedding_matrix)\n",
    "                embedded = tf.reshape(embedded, (-1, self.sentence_length, self.embed_dimension))\n",
    "\n",
    "                # dodajemy warstwy konwolucji, po ktorych następuje maxpool\n",
    "                layer1 = tf.layers.conv1d(embedded, 32, 5, padding='same', activation=tf.nn.relu)\n",
    "                # , name=\"conv1\", reuse=True)\n",
    "                layer2 = tf.layers.conv1d(layer1, 64, 3, padding='same', activation=tf.nn.relu)\n",
    "                pooling = tf.layers.max_pooling1d(layer2, 3, 2)\n",
    "\n",
    "                # zeby polaczyc kanaly wyplaszczamy dane, a nastepnie wrzucamy\n",
    "                # do zwyklej sieci fully connected\n",
    "                flattened = tf.contrib.layers.flatten(pooling)\n",
    "                layer3 = tf.layers.dense(flattened, 50)\n",
    "                score = tf.layers.dense(layer3, 1)\n",
    "                return score\n",
    "\n",
    "        scorer1 = get_output_tensor(inputs)\n",
    "        scorer2 = get_output_tensor(false_inputs)\n",
    "\n",
    "        loss = tf.reduce_mean(tf.maximum(tf.zeros_like(scorer1), 1 - (scorer1 - scorer2)))\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1e-2).minimize(loss)\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        \n",
    "        def chunk_once(doc, width):\n",
    "            chunks = [doc[i:i+width] for i in range(0, len(doc), width)]\n",
    "            chunks[-1] += [0] * (width - len(chunks[-1]))\n",
    "            return chunks\n",
    "\n",
    "        def chunk_and_padd(documents, width):\n",
    "            chunks = [w for d in documents for w in chunk_once(d, width)]\n",
    "            return chunks\n",
    "        \n",
    "        # kroi tablice na podtablice długości n\n",
    "        dataset = chunk_and_padd(train, self.sentence_length)\n",
    "\n",
    "        def noiser(sentence, p):\n",
    "            new_sent = [x for x in sentence]\n",
    "            for i, _ in enumerate(new_sent):\n",
    "                if np.random.random() < p:\n",
    "                    new_sent[i] = np.random.choice(NUM_WORDS-1)+1\n",
    "            return new_sent\n",
    "        epochs = 10\n",
    "        batch_size = 128\n",
    "        num_batches = len(dataset)//batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print('epoch: ' + str(epoch + 1) + \"/\" + str(epochs) )\n",
    "            for _ in range(num_batches):\n",
    "                batch_indexes = np.random.choice(len(dataset),batch_size)\n",
    "                batch = [dataset[i] for i in batch_indexes]\n",
    "                false_batch = [noiser(dataset[i],0.2) for i in batch_indexes]\n",
    "                sess.run(optimizer,feed_dict={inputs:batch, false_inputs:false_batch})\n",
    "\n",
    "        matrix = sess.run(embedding_matrix)\n",
    "        self.embedding = matrix\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def embed(self, sentence):\n",
    "        vectorized = self.vectorizer.transform([sentence]).toarray()[0]\n",
    "        result = vectorized.dot(self.embedding)\n",
    "        return result\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/10\n",
      "epoch: 2/10\n",
      "epoch: 3/10\n",
      "epoch: 4/10\n",
      "epoch: 5/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-489d378732e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEmbedder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-6bdbfcb52287>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# tutaj nastepuje uczenie embeddingu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0membedded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# upewnienie sie ze embedding ma staly wymiar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-3336bfc9c4e9>\u001b[0m in \u001b[0;36mtrain_embeddings\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_indexes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[0mfalse_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnoiser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_indexes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfalse_inputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfalse_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mmatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "model = LogisticRegressionModel(Embedder())\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print(model.score(X_train, Y_train))\n",
    "print(model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
